{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os, time, random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import Callable, List\n",
    "from functools import cache\n",
    "import re\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = Path('.').absolute()\n",
    "data_dir=cur_dir.parent/ 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/t/aproject/movie-recommender-system-collaborative-filtering/data/tag.csv'),\n",
       " PosixPath('/home/t/aproject/movie-recommender-system-collaborative-filtering/data/movie.csv'),\n",
       " PosixPath('/home/t/aproject/movie-recommender-system-collaborative-filtering/data/link.csv'),\n",
       " PosixPath('/home/t/aproject/movie-recommender-system-collaborative-filtering/data/rating.csv'),\n",
       " PosixPath('/home/t/aproject/movie-recommender-system-collaborative-filtering/data/movies.csv'),\n",
       " PosixPath('/home/t/aproject/movie-recommender-system-collaborative-filtering/data/genome_tags.csv'),\n",
       " PosixPath('/home/t/aproject/movie-recommender-system-collaborative-filtering/data/genome_scores.csv')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data_dir.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Movie df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df = pd.read_csv(data_dir/'movie.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27278 entries, 0 to 27277\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   movieId  27278 non-null  int64 \n",
      " 1   title    27278 non-null  object\n",
      " 2   genres   27278 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 639.5+ KB\n"
     ]
    }
   ],
   "source": [
    "movie_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                               title  \\\n",
       "0        1                    Toy Story (1995)   \n",
       "1        2                      Jumanji (1995)   \n",
       "2        3             Grumpier Old Men (1995)   \n",
       "3        4            Waiting to Exhale (1995)   \n",
       "4        5  Father of the Bride Part II (1995)   \n",
       "\n",
       "                                        genres  \n",
       "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
       "1                   Adventure|Children|Fantasy  \n",
       "2                               Comedy|Romance  \n",
       "3                         Comedy|Drama|Romance  \n",
       "4                                       Comedy  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movieId    0\n",
       "title      0\n",
       "genres     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_df.isna().sum() # no nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_df.duplicated().sum() #no duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27262"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_df.title.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aladdin (1992)', 'Johnny Express (2014)', 'Chaos (2005)', 'Hamlet (2000)', '20,000 Leagues Under the Sea (1997)', 'Darling (2007)', 'Casanova (2005)', 'Paradise (2013)', 'Beneath (2013)', 'Girl, The (2012)', 'Clear History (2013)', 'Emma (1996)', 'Offside (2006)', 'Blackout (2007)', 'Men with Guns (1997)', 'War of the Worlds (2005)']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate the value counts for each movie title\n",
    "title_value_counts = movie_df['title'].value_counts()\n",
    "\n",
    "# Filter titles that appear more than once\n",
    "duplicate_titles = title_value_counts[title_value_counts > 1].index.tolist()\n",
    "\n",
    "print(duplicate_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some movies have multiple entries with different `movieid` , but it doesn't affect much "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so in genres column there seems to have no spaces bw genres, lets see want unique genres as there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genres = movie_df.genres.apply(lambda x : ' '.join(str(x).split('|'))).values.tolist() # split from |\n",
    "all_genres = ' '.join(set(all_genres)).split() # join all strings and break them into words\n",
    "all_genres = set(all_genres)  # make a set to find unique ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Children', 'Horror', 'Action', 'Thriller', 'Mystery', 'Romance', 'Fantasy', 'Adventure', 'Comedy', '(no', 'Crime', 'IMAX', 'genres', 'listed)', 'War', 'Musical', 'Film-Noir', 'Animation', 'Drama', 'Documentary', 'Sci-Fi', 'Western'} 22\n"
     ]
    }
   ],
   "source": [
    "print(all_genres, len(all_genres))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are 20 genres and 1 for movies with no genre (which is (no listed)) which is broken as '(no' and 'listed)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a tag columns that contains all information about movie in a sentence\n",
    "movie_df['tags'] = movie_df['title'] + ' ' + movie_df['genres'].apply(lambda x: ' '.join(str(x).split('|')))\n",
    "movie_df['tags'] = movie_df['tags'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movie_df['Title'] = movie_df.title.apply(lambda x : str(x).split('(')[0])\n",
    "# movie_df['Year'] = movie_df.title.apply(lambda x : (str(x).split('(')[-1]).strip(')'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24223</th>\n",
       "      <td>114818</td>\n",
       "      <td>Stretch (2014)</td>\n",
       "      <td>Action|Comedy|Crime</td>\n",
       "      <td>stretch (2014) action comedy crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21202</th>\n",
       "      <td>103277</td>\n",
       "      <td>Hatfields &amp; McCoys (2012)</td>\n",
       "      <td>Drama|Romance</td>\n",
       "      <td>hatfields &amp; mccoys (2012) drama romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14443</th>\n",
       "      <td>72315</td>\n",
       "      <td>Hell's Hinges (1916)</td>\n",
       "      <td>Romance|Western</td>\n",
       "      <td>hell's hinges (1916) romance western</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15395</th>\n",
       "      <td>78465</td>\n",
       "      <td>Three Little Words (1950)</td>\n",
       "      <td>Comedy|Musical|Romance</td>\n",
       "      <td>three little words (1950) comedy musical romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18901</th>\n",
       "      <td>94024</td>\n",
       "      <td>Louis Theroux: The Most Hated Family in Americ...</td>\n",
       "      <td>Documentary</td>\n",
       "      <td>louis theroux: the most hated family in americ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18433</th>\n",
       "      <td>91929</td>\n",
       "      <td>Nostalgia for the Light (Nostalgia de la luz) ...</td>\n",
       "      <td>Documentary|Drama</td>\n",
       "      <td>nostalgia for the light (nostalgia de la luz) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12701</th>\n",
       "      <td>59838</td>\n",
       "      <td>Chato's Land (1972)</td>\n",
       "      <td>Western</td>\n",
       "      <td>chato's land (1972) western</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19056</th>\n",
       "      <td>94810</td>\n",
       "      <td>Eva (2011)</td>\n",
       "      <td>Drama|Fantasy|Sci-Fi</td>\n",
       "      <td>eva (2011) drama fantasy sci-fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7875</th>\n",
       "      <td>8530</td>\n",
       "      <td>Dear Frankie (2004)</td>\n",
       "      <td>Drama|Romance</td>\n",
       "      <td>dear frankie (2004) drama romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10185</th>\n",
       "      <td>33847</td>\n",
       "      <td>Home Movie (2001)</td>\n",
       "      <td>Documentary</td>\n",
       "      <td>home movie (2001) documentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5083</th>\n",
       "      <td>5179</td>\n",
       "      <td>Gloria (1980)</td>\n",
       "      <td>Drama|Thriller</td>\n",
       "      <td>gloria (1980) drama thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364</th>\n",
       "      <td>5461</td>\n",
       "      <td>Me Without You (2001)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "      <td>me without you (2001) comedy drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18145</th>\n",
       "      <td>90845</td>\n",
       "      <td>Fall of the House of Usher, The (Zánik domu Us...</td>\n",
       "      <td>Animation</td>\n",
       "      <td>fall of the house of usher, the (zánik domu us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12022</th>\n",
       "      <td>54378</td>\n",
       "      <td>Man's Job (Miehen työ) (2007)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>man's job (miehen työ) (2007) drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3321</th>\n",
       "      <td>3409</td>\n",
       "      <td>Final Destination (2000)</td>\n",
       "      <td>Drama|Thriller</td>\n",
       "      <td>final destination (2000) drama thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>1501</td>\n",
       "      <td>Keys to Tulsa (1997)</td>\n",
       "      <td>Crime</td>\n",
       "      <td>keys to tulsa (1997) crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19047</th>\n",
       "      <td>94784</td>\n",
       "      <td>Olympian Holiday (Loma) (1976)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "      <td>olympian holiday (loma) (1976) comedy romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10070</th>\n",
       "      <td>33229</td>\n",
       "      <td>Angry Silence, The (1960)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>angry silence, the (1960) drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12313</th>\n",
       "      <td>56693</td>\n",
       "      <td>Close My Eyes (1991)</td>\n",
       "      <td>Drama|Romance</td>\n",
       "      <td>close my eyes (1991) drama romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17632</th>\n",
       "      <td>88676</td>\n",
       "      <td>Divine Horsemen: The Living Gods of Haiti (1985)</td>\n",
       "      <td>Documentary</td>\n",
       "      <td>divine horsemen: the living gods of haiti (198...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5733</th>\n",
       "      <td>5832</td>\n",
       "      <td>Left Behind II: Tribulation Force (2002)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>left behind ii: tribulation force (2002) drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7161</th>\n",
       "      <td>7273</td>\n",
       "      <td>Piece of the Action, A (1977)</td>\n",
       "      <td>Crime|Drama</td>\n",
       "      <td>piece of the action, a (1977) crime drama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       movieId                                              title  \\\n",
       "24223   114818                                     Stretch (2014)   \n",
       "21202   103277                          Hatfields & McCoys (2012)   \n",
       "14443    72315                               Hell's Hinges (1916)   \n",
       "15395    78465                          Three Little Words (1950)   \n",
       "18901    94024  Louis Theroux: The Most Hated Family in Americ...   \n",
       "18433    91929  Nostalgia for the Light (Nostalgia de la luz) ...   \n",
       "12701    59838                                Chato's Land (1972)   \n",
       "19056    94810                                         Eva (2011)   \n",
       "7875      8530                                Dear Frankie (2004)   \n",
       "10185    33847                                  Home Movie (2001)   \n",
       "5083      5179                                      Gloria (1980)   \n",
       "5364      5461                              Me Without You (2001)   \n",
       "18145    90845  Fall of the House of Usher, The (Zánik domu Us...   \n",
       "12022    54378                      Man's Job (Miehen työ) (2007)   \n",
       "3321      3409                           Final Destination (2000)   \n",
       "1458      1501                               Keys to Tulsa (1997)   \n",
       "19047    94784                     Olympian Holiday (Loma) (1976)   \n",
       "10070    33229                          Angry Silence, The (1960)   \n",
       "12313    56693                               Close My Eyes (1991)   \n",
       "17632    88676   Divine Horsemen: The Living Gods of Haiti (1985)   \n",
       "5733      5832           Left Behind II: Tribulation Force (2002)   \n",
       "7161      7273                      Piece of the Action, A (1977)   \n",
       "\n",
       "                       genres  \\\n",
       "24223     Action|Comedy|Crime   \n",
       "21202           Drama|Romance   \n",
       "14443         Romance|Western   \n",
       "15395  Comedy|Musical|Romance   \n",
       "18901             Documentary   \n",
       "18433       Documentary|Drama   \n",
       "12701                 Western   \n",
       "19056    Drama|Fantasy|Sci-Fi   \n",
       "7875            Drama|Romance   \n",
       "10185             Documentary   \n",
       "5083           Drama|Thriller   \n",
       "5364             Comedy|Drama   \n",
       "18145               Animation   \n",
       "12022                   Drama   \n",
       "3321           Drama|Thriller   \n",
       "1458                    Crime   \n",
       "19047          Comedy|Romance   \n",
       "10070                   Drama   \n",
       "12313           Drama|Romance   \n",
       "17632             Documentary   \n",
       "5733                    Drama   \n",
       "7161              Crime|Drama   \n",
       "\n",
       "                                                    tags  \n",
       "24223                 stretch (2014) action comedy crime  \n",
       "21202            hatfields & mccoys (2012) drama romance  \n",
       "14443               hell's hinges (1916) romance western  \n",
       "15395   three little words (1950) comedy musical romance  \n",
       "18901  louis theroux: the most hated family in americ...  \n",
       "18433  nostalgia for the light (nostalgia de la luz) ...  \n",
       "12701                        chato's land (1972) western  \n",
       "19056                    eva (2011) drama fantasy sci-fi  \n",
       "7875                   dear frankie (2004) drama romance  \n",
       "10185                      home movie (2001) documentary  \n",
       "5083                        gloria (1980) drama thriller  \n",
       "5364                  me without you (2001) comedy drama  \n",
       "18145  fall of the house of usher, the (zánik domu us...  \n",
       "12022                man's job (miehen työ) (2007) drama  \n",
       "3321             final destination (2000) drama thriller  \n",
       "1458                          keys to tulsa (1997) crime  \n",
       "19047      olympian holiday (loma) (1976) comedy romance  \n",
       "10070                    angry silence, the (1960) drama  \n",
       "12313                 close my eyes (1991) drama romance  \n",
       "17632  divine horsemen: the living gods of haiti (198...  \n",
       "5733      left behind ii: tribulation force (2002) drama  \n",
       "7161           piece of the action, a (1977) crime drama  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_df.sample(22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploraing User data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = pd.read_csv(data_dir/'rating.csv', usecols=['userId','movieId','rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138493, 1, 138493)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df.userId.max(), user_df.userId.min(), user_df.userId.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# this columns are using too much precision for very low values, lowering the datatype precision\n",
    "user_df['movieId'] = user_df['movieId'].astype('int32')\n",
    "user_df['userId'] = user_df['userId'].astype('int32')\n",
    "user_df['rating'] = user_df['rating'].astype('float32')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138493, 1, 138493)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df.userId.max(), user_df.userId.min(), user_df.userId.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000263, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df.shape #(20000263,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000263 entries, 0 to 20000262\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Dtype  \n",
      "---  ------   -----  \n",
      " 0   userId   int32  \n",
      " 1   movieId  int32  \n",
      " 2   rating   float32\n",
      "dtypes: float32(1), int32(2)\n",
      "memory usage: 228.9 MB\n"
     ]
    }
   ],
   "source": [
    "user_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating\n",
       "0       1        2     3.5\n",
       "1       1       29     3.5\n",
       "2       1       32     3.5\n",
       "3       1       47     3.5\n",
       "4       1       50     3.5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(x: str) -> str:\n",
    "    x = re.sub(r'[^\\w\\s]', '', x)  # Remove punctuation\n",
    "    x = x.lower()  # Convert to lowercase\n",
    "    return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'helo 44 sir'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'helo#$#@$#%$@%@#$ 44 sir'\n",
    "clean_text(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, special_tokens: List[str]):\n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = {}\n",
    "        self.counter = Counter()\n",
    "        \n",
    "        self.UNK_TOKEN = '<UNK>'\n",
    "        self.UNK = 1\n",
    "        self.PAD_TOKEN = '<PAD>'\n",
    "        self.PAD = 0\n",
    "        \n",
    "        self.word_to_idx[self.UNK_TOKEN] = self.UNK\n",
    "        self.idx_to_word[self.UNK] = self.UNK_TOKEN\n",
    "\n",
    "        self.word_to_idx[self.PAD_TOKEN] = self.PAD\n",
    "        self.idx_to_word[self.PAD] = self.PAD_TOKEN\n",
    "\n",
    "        self.vocab_size = 2\n",
    "        for idx, token in enumerate(special_tokens, start=2):\n",
    "            self.word_to_idx[token] = idx\n",
    "            self.idx_to_word[idx] = token\n",
    "            self.vocab_size += 1\n",
    "\n",
    "    def build_vocab(self, tokenized_data, max_tokens, min_freq):\n",
    "        self.counter = Counter()\n",
    "        for words in tokenized_data:\n",
    "            self.counter.update(words)\n",
    "\n",
    "        if max_tokens is not None:\n",
    "            sorted_tokens = [word for word, _ in self.counter.most_common()]\n",
    "            for word in sorted_tokens:\n",
    "                if word not in self.word_to_idx:\n",
    "                    self.add_word_to_vocab(word)\n",
    "\n",
    "                if self.vocab_size == max_tokens - 1:\n",
    "                    break\n",
    "        else:\n",
    "            for word, freq in self.counter.items():\n",
    "                if freq >= min_freq and word not in self.word_to_idx:\n",
    "                    self.add_word_to_vocab(word)\n",
    "\n",
    "    def add_word_to_vocab(self, word):\n",
    "        self.word_to_idx[word] = self.vocab_size\n",
    "        self.idx_to_word[self.vocab_size] = word\n",
    "        self.vocab_size += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class IntegerVectorizer:\n",
    "    def __init__(self, \n",
    "                 tokenizer: Callable[[str], List[str]] = None,\n",
    "                 preprocessing_func: Callable[[str], str] = None,\n",
    "                 max_tokens=None,\n",
    "                 min_freq=1,\n",
    "                 special_tokens: List[str] = None,\n",
    "                 max_seq_length=None,\n",
    "                 pad_to_max=False):\n",
    "        \n",
    "        self.min_freq = min_freq\n",
    "        self.max_tokens = max_tokens\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.preprocessing_func = preprocessing_func\n",
    "        self.reserved_tokens = ['<UNK>', '<PAD>']\n",
    "        self.special_tokens = [token for token in special_tokens if token not in self.reserved_tokens] if special_tokens else []\n",
    "        self.pad_to_max = pad_to_max  # Store the argument\n",
    "\n",
    "        self.vocab = Vocabulary(self.special_tokens)\n",
    "        self.tokenized_data = []\n",
    "\n",
    "    def adapt(self, data):\n",
    "        self.tokenized_data = self.tokenize_data_generator(data)\n",
    "        self.vocab.build_vocab(self.tokenized_data, self.max_tokens, self.min_freq)\n",
    "        print('Vocab size:', len(self.vocab))\n",
    "\n",
    "    def __call__(self, data, reverse=False):\n",
    "        if reverse:\n",
    "            return self.reverse_transform(data)\n",
    "        else:\n",
    "            return self.transform(data)\n",
    "\n",
    "    def preprocess_sentence(self, sentence):\n",
    "        if self.preprocessing_func:\n",
    "            words = sentence.split()\n",
    "            preprocessed_words = [self.preprocessing_func(word) if word not in self.special_tokens else word for word in words]\n",
    "            return \" \".join(preprocessed_words)\n",
    "        return sentence\n",
    "\n",
    "    def tokenize_data_generator(self, data):\n",
    "        for sentence in data:\n",
    "            sentence = self.preprocess_sentence(sentence)\n",
    "            words = self.tokenizer(sentence) if self.tokenizer else sentence.split()\n",
    "            yield words\n",
    "\n",
    "    def transform(self, data):\n",
    "        self.tokenized_data = self.tokenize_data_generator(data)\n",
    "        vectorized_data = []\n",
    "        for sentence in self.tokenized_data:\n",
    "            vectorized_sentence = [self.vocab.word_to_idx.get(word, self.vocab.UNK) for word in sentence]\n",
    "            vectorized_sentence = self.adjust_sequence_length(vectorized_sentence)\n",
    "            vectorized_data.append(vectorized_sentence)\n",
    "        return vectorized_data\n",
    "\n",
    "    def adjust_sequence_length(self, sequence):\n",
    "        if self.max_seq_length is not None:\n",
    "            if len(sequence) < self.max_seq_length:\n",
    "                if self.pad_to_max:  # Check the new argument\n",
    "                    sequence += [self.vocab.PAD] * (self.max_seq_length - len(sequence))\n",
    "            elif len(sequence) > self.max_seq_length:\n",
    "                sequence = sequence[:self.max_seq_length]\n",
    "        return sequence\n",
    "\n",
    "\n",
    "    def reverse_transform(self, vectorized_data: List[List[int]]) -> List[str]:\n",
    "        original_data = []\n",
    "        for vector in vectorized_data:\n",
    "            sentence = [self.vocab.idx_to_word[idx] for idx in vector if idx != self.vocab.PAD]\n",
    "            original_data.append(\" \".join(sentence))\n",
    "        return original_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 14\n",
      "Original data: ['This is a sample sentence (23). <UNK>', '<START> Another example sentence. <END>', '<START> This is another sentence. <END>', '3 3 3 3 3 3 ']\n",
      "Vectorized data: [[2, 3, 4, 5, 6, 7, 8], [9, 10, 11, 6, 12], [9, 2, 3, 10, 6, 12], [13, 13, 13, 13, 13, 13]]\n",
      "reverse vec:  ['this is a sample sentence 23 unk', 'start another example sentence end', 'start this is another sentence end', '3 3 3 3 3 3']\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    \"This is a sample sentence (23). <UNK>\",\n",
    "    \"<START> Another example sentence. <END>\",\n",
    "    \"<START> This is another sentence. <END>\",\n",
    "    '3 3 3 3 3 3 '\n",
    "]\n",
    "min_freq = 2\n",
    "max_tokens = None\n",
    "special_tokens = ['<START>', '<END>', '<UNK>','<PAD>']\n",
    "\n",
    "\n",
    "vectorizer = IntegerVectorizer(preprocessing_func=clean_text, max_seq_length=10, pad_to_max=False)\n",
    "vectorizer.adapt(data)\n",
    "a = vectorizer(data)\n",
    "b = vectorizer.reverse_transform(a)\n",
    "\n",
    "\n",
    "print(\"Original data:\", data)\n",
    "print(\"Vectorized data:\", a)\n",
    "print('reverse vec: ', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       user_df: 228.9 MiB\n",
      "                      movie_df:  7.0 MiB\n",
      "            title_value_counts:  2.5 MiB\n",
      "                           _15:  6.0 KiB\n",
      "                           _ii:  3.0 KiB\n",
      "                          _i26:  3.0 KiB\n",
      "                    all_genres:  2.2 KiB\n",
      "                       Dataset:  1.6 KiB\n",
      "                    DataLoader:  1.6 KiB\n",
      "                          tqdm:  1.6 KiB\n"
     ]
    }
   ],
   "source": [
    "def sizeof_fmt(num, suffix=\"B\"):\n",
    "    \n",
    "    \"\"\"by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified\"\"\"\n",
    "    for unit in [\"\", \"Ki\", \"Mi\", \"Gi\", \"Ti\", \"Pi\", \"Ei\", \"Zi\"]:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, \"Yi\", suffix)\n",
    "    \n",
    "\n",
    "for name, size in sorted(\n",
    "    ((name, sys.getsizeof(value)) for name, value in locals().items()),\n",
    "    key=lambda x: -x[1],\n",
    ")[:10]:\n",
    "    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class MovieRatingDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        user_id = self.dataframe.iloc[idx]['userId']\n",
    "        movie_tags = torch.tensor(self.dataframe.iloc[idx]['tagvector'], dtype=torch.long)\n",
    "        # movie_tags = self.dataframe.iloc[idx]['movieId']\n",
    "        rating = self.dataframe.iloc[idx]['rating']        \n",
    "        return user_id, movie_tags, rating\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "\n",
    "class RecommenderModel(nn.Module):\n",
    "    def __init__(self, num_users, num_tokens, embedding_dim, model_path:Path=None):\n",
    "        super(RecommenderModel, self).__init__()\n",
    "        self.model_path = model_path\n",
    "        self.user_embedding = nn.Embedding(num_users+1, embedding_dim)\n",
    "        self.movie_embedding = nn.Embedding(num_tokens+1, embedding_dim)\n",
    "        self.out = nn.Linear(embedding_dim, 1)\n",
    "        \n",
    "    def forward(self, user_ids, movie_tags, debug=False):\n",
    "        user_ids = user_ids.to(torch.long)  # Convert to Long data type\n",
    "        movie_tags = movie_tags.to(torch.long)  # Convert to Long data type\n",
    "\n",
    "        user_emb = self.user_embedding(user_ids)\n",
    "        movie_emb = self.movie_embedding(movie_tags)\n",
    "        interaction = user_emb * movie_emb\n",
    "        x = interaction.mean(dim=1)\n",
    "        output = self.out(x)\n",
    "\n",
    "        if debug:\n",
    "            print('user_emb.shape: ',user_emb.shape)\n",
    "            print('movie_emb.shape: ',movie_emb.shape)\n",
    "            print('interaction.shape: ',interaction.shape)\n",
    "            print('output.shape:',output.shape)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def load_model(self, model_path=None):\n",
    "        if model_path is None:\n",
    "            model_path = self.model_path\n",
    "        if Path(model_path).is_file():\n",
    "            self.load_state_dict(torch.load(model_path))\n",
    "            print('Model weights loaded.')\n",
    "        else:\n",
    "            print('Weights not found.')\n",
    "            \n",
    "    def save_model(self, model_path=None):\n",
    "        if model_path is None:\n",
    "            model_path = self.model_path\n",
    "        torch.save(self.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecommenderModel(\n",
       "  (user_embedding): Embedding(11, 8)\n",
       "  (movie_embedding): Embedding(21, 8)\n",
       "  (out): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RecommenderModel(10, 20, 8)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_out = model(torch.randint(1,10,(8,1)), torch.randint(1,20,(8,1)))\n",
    "test_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, loss_function, num_epochs=10, device='cpu', data_percent=1.0, steps_per_epoch=None):\n",
    "    model.to(device)\n",
    "    print(f'{model.__class__.__name__} Running on: {device}')\n",
    "\n",
    "    data_size = int(data_percent * len(dataloader))\n",
    "    dataloader = iter(dataloader)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        total_mse = 0.0\n",
    "        total_mae = 0.0\n",
    "        total_samples = 0\n",
    "\n",
    "        epoch_progress = tqdm(range(data_size), desc=f\"Epoch [{epoch+1:2}/{num_epochs:2}]\")\n",
    "        \n",
    "        if steps_per_epoch is not None:\n",
    "            epoch_progress = tqdm(range(steps_per_epoch), desc=f\"Epoch [{epoch+1:2}/{num_epochs:2}]\")\n",
    "\n",
    "        last_update_time = time.time() - 1.0  # Initialize to ensure the first update\n",
    "        \n",
    "        for _ in epoch_progress:\n",
    "            try:\n",
    "                batch = next(dataloader)\n",
    "            except StopIteration:\n",
    "                dataloader = iter(dataloader)\n",
    "                batch = next(dataloader)\n",
    "\n",
    "            user_ids, movie_tags, ratings = batch\n",
    "\n",
    "            user_ids = user_ids.view(-1, 1)\n",
    "\n",
    "            user_ids = user_ids.to(device)\n",
    "            movie_tags = movie_tags.to(device)\n",
    "            ratings = ratings.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(user_ids, movie_tags).squeeze()\n",
    "\n",
    "            loss = loss_function(outputs, ratings)\n",
    "            \n",
    "            mse = F.mse_loss(outputs, ratings)\n",
    "            mae = F.l1_loss(outputs, ratings)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_mse += mse.item()\n",
    "            total_mae += mae.item()\n",
    "            total_samples += len(ratings)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            formatted_loss = f\"{loss.item():.8f}\"\n",
    "            formatted_mse = f\"{mse.item():.8f}\"\n",
    "            formatted_mae = f\"{mae.item():.8f}\"\n",
    "            \n",
    "            current_time = time.time()\n",
    "            if current_time - last_update_time > epoch_progress.mininterval:\n",
    "                epoch_progress.set_postfix({\"Loss\": formatted_loss, \"MSE\": formatted_mse, \"MAE\": formatted_mae})\n",
    "                epoch_progress.update()\n",
    "                last_update_time = current_time\n",
    "\n",
    "            if steps_per_epoch is not None and _ + 1 >= steps_per_epoch:\n",
    "                break\n",
    "\n",
    "        # epoch_progress.close()\n",
    "        average_loss = total_loss / min(data_size, steps_per_epoch) if steps_per_epoch is not None else total_loss / data_size\n",
    "        average_mse = total_mse / min(data_size, steps_per_epoch) if steps_per_epoch is not None else total_mse / data_size\n",
    "        average_mae = total_mae / min(data_size, steps_per_epoch) if steps_per_epoch is not None else total_mae / data_size\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1:2}/{num_epochs:2}] - Average Loss: {average_loss:.8f} - Average MSE: {average_mse:.8f} - Average MAE: {average_mae:.8f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader, device='cpu'):\n",
    "    model.to(device)\n",
    "    \n",
    "    labels = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch in dataloader:\n",
    "            user_ids, movie_tags, ratings = batch\n",
    "            \n",
    "            user_ids = user_ids.view(-1, 1)\n",
    "\n",
    "            user_ids = user_ids.to(device)\n",
    "            movie_tags = movie_tags.to(device)\n",
    "            ratings = ratings.to(device)\n",
    "\n",
    "            outputs = model(user_ids, movie_tags).squeeze()\n",
    "\n",
    "            labels.extend(ratings.cpu().numpy())\n",
    "            \n",
    "            if len(outputs.shape) == 0:\n",
    "                predictions.append(outputs.item())\n",
    "            else:\n",
    "                predictions.extend(outputs.cpu().numpy())\n",
    "\n",
    "    print('Mean Squared Error : ',mean_squared_error(labels, predictions))\n",
    "    print('Mean Absolute Error: ',mean_absolute_error(labels, predictions))\n",
    "    return labels, predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 22693\n"
     ]
    }
   ],
   "source": [
    "#adapt to data\n",
    "vectorizer = IntegerVectorizer(preprocessing_func=clean_text,max_seq_length=10, pad_to_max=True)\n",
    "vectorizer.adapt(movie_df['tags'].tolist()) #22693\n",
    "\n",
    "# vectorize the data\n",
    "movie_df['tagvector'] = movie_df['tags'].apply(lambda x : vectorizer.transform([x])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge movie_df to user_df\n",
    "user_df = user_df.merge(movie_df[['movieId','tagvector']], on='movieId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138493, 1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df.userId.max(), user_df.userId.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User IDs: tensor([ 22848, 107608,  95501,  60610,  20314,  51668,   6983,  24019,  18547,\n",
      "         80067,  73074, 130322,  14457, 129058,  78927,  56324,   6228, 116287,\n",
      "         44527,  67224,  91948,  71975,  54059, 133583, 125794, 132219, 115013,\n",
      "          3085,  54024,  26004,  67073,   5768], dtype=torch.int32)\n",
      "Movie Tags: tensor([[ 2203,  1140,     8,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  956,   957,   421,    26,     5,    65,    28,     0,     0,     0],\n",
      "        [   11,    12,    13,     4,     8,    14,     0,     0,     0,     0],\n",
      "        [  664,  1856,  5481,    27,    62,    28,     0,     0,     0,     0],\n",
      "        [   74,   153,  7306,  6654,     8,    18,    14,     0,     0,     0],\n",
      "        [  613,   153,    21,   402,     4,    18,    28,     0,     0,     0],\n",
      "        [ 2053,    16,    21,  2187,  1061,     5,     8,    65,     0,     0],\n",
      "        [ 2174,  1807,    39,  1061,     8,    14,     0,     0,     0,     0],\n",
      "        [  701,    20,  1208,  6075,    21,  5481,    18,    28,     0,     0],\n",
      "        [ 1238,  1383,  1384,     4,     8,     0,     0,     0,     0,     0],\n",
      "        [ 1490,  1491,    21,   156,     8,     9,    14,    65,     0,     0],\n",
      "        [10683,  9681,    27,    18,     0,     0,     0,     0,     0,     0],\n",
      "        [ 1163,   591,   219,  1378,     8,     0,     0,     0,     0,     0],\n",
      "        [  138,  9882, 11590,     8,     0,     0,     0,     0,     0,     0],\n",
      "        [  892,   421,     8,    14,     0,     0,     0,     0,     0,     0],\n",
      "        [ 2003,    20,  4975,    21,  1246,  7800,   771, 11101,  9890,    18],\n",
      "        [  505,   506,   507,     4,     8,    18,    14,     0,     0,     0],\n",
      "        [  492,    98,   356,    18,     0,     0,     0,     0,     0,     0],\n",
      "        [  555,  1172,  1173,    26,     5,     6,    42,    65,     0,     0],\n",
      "        [ 2122,  1812,    42,    65,     0,     0,     0,     0,     0,     0],\n",
      "        [  989,   566,    21,  2133,    18,     0,     0,     0,     0,     0],\n",
      "        [  118,    21,   348, 12097,   261,   855, 11590,   103,     0,     0],\n",
      "        [ 4856,  2703,     8,    27,    28,     0,     0,     0,     0,     0],\n",
      "        [  117,   396,   174,  1131,    26,     5,    28,     0,     0,     0],\n",
      "        [ 3198,   881,  1664,    42,    28,     0,     0,     0,     0,     0],\n",
      "        [  123,    91,   124,     4,    62,    28,     0,     0,     0,     0],\n",
      "        [ 4865,   870,  2703,    18,     0,     0,     0,     0,     0,     0],\n",
      "        [  801,   134,    26,    14,    28,     0,     0,     0,     0,     0],\n",
      "        [  926,  1161,  7774,  6654,     8,    18,     0,     0,     0,     0],\n",
      "        [11035, 10065,    18,    28,     0,     0,     0,     0,     0,     0],\n",
      "        [ 3584,  3700,  6654,     5,     6,     7,     8,     0,     0,     0],\n",
      "        [ 7670,  6654,    26,    65,    28,     0,     0,     0,     0,     0]])\n",
      "Ratings: tensor([4.5000, 3.5000, 2.5000, 3.5000, 4.5000, 3.0000, 5.0000, 3.5000, 4.0000,\n",
      "        5.0000, 2.0000, 4.0000, 5.0000, 4.5000, 4.0000, 4.5000, 4.0000, 3.0000,\n",
      "        4.0000, 4.5000, 1.5000, 3.5000, 5.0000, 3.0000, 4.0000, 4.0000, 5.0000,\n",
      "        4.0000, 3.5000, 4.0000, 4.0000, 2.5000])\n"
     ]
    }
   ],
   "source": [
    "# Set batch size for DataLoader\n",
    "batch_size = 32\n",
    "\n",
    "# train\n",
    "dataset = MovieRatingDataset(user_df[['userId','tagvector','rating', 'movieId']])\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Iterate through the DataLoader during training\n",
    "for batch in dataloader:\n",
    "    user_ids, movie_tags, ratings = batch\n",
    "    print(\"User IDs:\", user_ids)\n",
    "    print(\"Movie Tags:\", movie_tags)\n",
    "    print(\"Ratings:\", ratings)\n",
    "    break  # only print the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138493, 27278, 22693)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df.userId.nunique(),movie_df.movieId.nunique(), len(vectorizer.vocab.word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max userid value:  138493\n",
      "unique userid:  138493\n"
     ]
    }
   ],
   "source": [
    "print('max userid value: ',user_df.userId.max())\n",
    "print('unique userid: ',user_df.userId.nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max vocab value:  22692\n",
      "vocab size:  22693\n"
     ]
    }
   ],
   "source": [
    "print('max vocab value: ',max(vectorizer.vocab.word_to_idx.values()))\n",
    "print('vocab size: ',len(vectorizer.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your model, optimizer, and loss function\n",
    "num_users = user_df.userId.nunique()  # actual number of users\n",
    "num_tokens = len(vectorizer.vocab) # actual number of tokens\n",
    "dim = 8\n",
    "model = RecommenderModel(num_users, num_tokens, dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1  # Set the number of training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model path\n",
    "model_dir = cur_dir.parent/'models'\n",
    "model_path = model_dir/'model.pth'\n",
    "model.model_path=model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights loaded.\n"
     ]
    }
   ],
   "source": [
    "# load the model is exists\n",
    "model.load_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecommenderModel Running on: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [ 1/ 2]: 100%|██████████| 6250/6250 [00:54<00:00, 114.56it/s, Loss=1.03188562, MSE=1.03188562, MAE=0.81621957]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 1/ 2] - Average Loss: 1.10505354 - Average MSE: 1.10505354 - Average MAE: 0.84157159\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [ 2/ 2]: 100%|██████████| 6250/6250 [00:51<00:00, 121.66it/s, Loss=1.08299732, MSE=1.08299732, MAE=0.85797203]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 2/ 2] - Average Loss: 1.10802470 - Average MSE: 1.10802470 - Average MAE: 0.84283979\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model, dataloader,  optimizer, loss_function, num_epochs=2, device=device, data_percent=0.01, steps_per_epoch=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels, predictions = predict(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22694, 8)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_movie_embedding = model.movie_embedding.weight.data.cpu().numpy()\n",
    "trained_movie_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8,),\n",
       " array([ 0.8688538 , -1.2884474 ,  0.07753823,  0.19225246,  0.9911368 ,\n",
       "         0.50074494,  0.27090523, -0.04132108], dtype=float32))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = trained_movie_embedding[0]\n",
    "a.shape, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t/miniconda3/envs/deep/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=10,random_state=0).fit(trained_movie_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 7, 3, ..., 1, 6, 9], dtype=int32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cluster in range(10):\n",
    "#     print('Cluster: ',cluster)\n",
    "#     movs = []\n",
    "    \n",
    "#     for movidx in np.where(kmeans.labels_==cluster)[0]:\n",
    "#         print(movidx)\n",
    "#         movie_id, movie_vector, movie_rating = train_dataset.__getitem__(movidx)\n",
    "#         print(movie_vector)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[39mreturn\u001b[39;00m top_indices\n\u001b[1;32m     20\u001b[0m target_movie_id \u001b[39m=\u001b[39m \u001b[39m72\u001b[39m  \u001b[39m# Replace with the target movie's ID\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m target_movie_embedding \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mmovie_embedding(torch\u001b[39m.\u001b[39;49mtensor(target_movie_id))\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     22\u001b[0m all_movie_embeddings \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mmovie_embedding\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdata\n\u001b[1;32m     23\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mall_movie_embeddings: \u001b[39m\u001b[39m'\u001b[39m,all_movie_embeddings\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/envs/deep/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/deep/lib/python3.11/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/miniconda3/envs/deep/lib/python3.11/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def find_similar_movies(target_movie_embedding, all_movie_embeddings, top_n=5):\n",
    "    with torch.inference_mode():\n",
    "        # Calculate cosine similarity\n",
    "        # print(target_movie_embedding.shape)\n",
    "        # print(all_movie_embeddings.shape)\n",
    "        similarity_scores = F.cosine_similarity(target_movie_embedding, all_movie_embeddings, dim=1)\n",
    "        \n",
    "        # print('smilarity score')\n",
    "        # # Sort movies based on similarity scores\n",
    "        sorted_indices = torch.argsort(similarity_scores, descending=True)\n",
    "        \n",
    "        # # Get top N similar movie indices\n",
    "        top_indices = sorted_indices[:top_n]\n",
    "        \n",
    "        # return top_indices\n",
    "        return top_indices\n",
    "\n",
    "target_movie_id = 72  # Replace with the target movie's ID\n",
    "target_movie_embedding = model.movie_embedding(torch.tensor(target_movie_id)).unsqueeze(0)\n",
    "all_movie_embeddings = model.movie_embedding.weight.data\n",
    "print('all_movie_embeddings: ',all_movie_embeddings.shape)\n",
    "\n",
    "# Find similar movies\n",
    "similar_movie_indices = find_similar_movies(target_movie_embedding, all_movie_embeddings, top_n=51)\n",
    "\n",
    "# Print or use the similar movie indices\n",
    "print(\"Similar movie indices:\", similar_movie_indices.shape)\n",
    "print(\"Similar movie indices:\", similar_movie_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_name(idx):\n",
    "    return movie_df[movie_df.movieId==idx].title.values[0]\n",
    "\n",
    "def get_movie_id(movie_name):\n",
    "    return movie_df[movie_df.title==movie_name].movieId.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = get_movie_name(4)\n",
    "b = get_movie_id(a)\n",
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def more_movies(target_movie_id):\n",
    "    print(get_movie_name(target_movie_id))\n",
    "    target_movie_embedding = model.movie_embedding(torch.tensor(target_movie_id)).unsqueeze(0)\n",
    "    all_movie_embeddings = model.movie_embedding.weight.data\n",
    "\n",
    "    # Find similar movies\n",
    "    similar_movie_indices = find_similar_movies(target_movie_embedding, all_movie_embeddings, top_n=50)\n",
    "    # Print or use the similar movie indices\n",
    "    for num, i in enumerate(similar_movie_indices,1):\n",
    "        try:\n",
    "            print(f\"{i} :{get_movie_name(int(i.numpy()))}\")\n",
    "        except IndexError :\n",
    "            print(f'Error at : {i.numpy()}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_movie_id('War, Inc. (2008)')\n",
    "get_movie_name(get_movie_id('War, Inc. (2008)'))\n",
    "more_movies(get_movie_id('War, Inc. (2008)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df.sample(33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df.movieId.max(), movie_df.movieId.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
